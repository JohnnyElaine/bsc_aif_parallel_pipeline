# Description
This python project implements a distributed parallel pipeline for processing streaming data in edge computing environments.
The main idea is to split a data stream into smaller chunks that are distributed across multiple workers (edge nodes) running in parallel. 
Each worker node processes its chunk, and the results are aggregated by a collector node to form a modified output stream.

The architecture draws inspiration from a [Apache Storm Topology](https://storm.apache.org/releases/2.7.1/Tutorial.html) and [Apache Flink](https://flink.apache.org/), consisting of:
A single Sprout (The collector), an internal layer of Bolts (Workers) and an aggregate Bolt (Collector).


For evaluation, a video stream of a highway is used as input, with YOLOv11 running inference in order to detect vehicles on live video frames. 
However, the framework is generic and applicable to any parallelizable data stream, such as audio, LIDAR, RADAR, network traffic, or sensor data.
**Central Topic**
"How to use Active Inference to uphold Service Level Objectives in edge computing scenarios"
**Central Research Question:**
"How can i efficiently process data streams in resource limited edge computing cases"
This is done by:
1. Implementing a distributed pipeline for parallel processing of data.
2. Employing Service Level Objectives (SLOs) in order uphold quality standards and abide by resource constraints
3. Implement elasticity in order to dynamically scale certain stream paramters in order to control the computational demand.
4. implementing an active inference (AIF) based agent in order to control elasticity demands

## Motivation
Edge computing offers scalable, low-latency processing closer to data sources, but is constrained by limited resources. 
This pipeline supports elasticity—the ability to dynamically adjust stream quality parameters in order to uphold a certain level of Quality of Experience (QoE) / Quality of Service (QoS) depending on the stream type, even when resources are constraint.
Elasticity specifically meaning to ability to dynamically manipulate certain quality parameters of the datastream in order to control overall computational load on the worker-nodes. 
Because we are in an edge computing scenario, we can't simply add more worker nodes when computational demand is high. 
This means that if the output stream of the parallel pipeline is not up to predefined QoE standard, then the stream must reduce quality parameters of some kind in order to reduce overall computational load on the system.

The decision making process regarding the elasticity is implemented using an Active Inference (AIF) based Agent. 
Active Inference (AIF) is an emerging concept from neuroscience that describes how the brain continuously predicts and evaluates sensory information to model real-world processes.
The goal if the AIF agent is to maximize the quality metrics of the output-stream while providing a predefined level of QoE. 
This delicate balance is implemented using certain preference regarding the quality metrics of the stream and Service Level Objectives (SLOs) in order to keep resource usage within bounds.

## Architecture
The pipeline consists of three main components:

- Producer: Generates and controls task flow.
- Worker: Processes tasks (e.g., YOLO inference).
- Collector: Aggregates processed results.

Throughout this README, examples refer to video streams and YOLOv11 inference, but the pipeline is general-purpose.

## Producer
The Producer continuously generates a stream of tasks. A ``Task`` object represents a single unit of work (e.g. a video frame).

### Task
A `Task` consists of 3 fields:
- `id` The `integer` id of the task
- `type` The `string` type of tasks. e.g. YOLOv11 inference
- `data` A `numpy.ndarray` containing a single frame of the video

#### Task Types
The worker nodes strictly adhere to the instructions stated within a given ``Task``. The general task types are:
- INFERENCE: Standard processing of frames using (YOLOv11) inference. Generated by the producer and carried out by the worker.
- COLLECT:  Standard collection of frames to re-assemble the data stream. Generated by the worker and carried out by the collector.
- CHANGE_INFERENCE_QUALITY: Change in inference quality. Switch (YOLOv11) model. Generated by the producer and carried out by the worker.
- END: End of the video stream. Propagate the END message and initiate graceful shutdown. Flow: Producer -> Worker -> Collector stopping all associated nodes in the process.

For example, when running inference on a video stream the producer would produce tasks such as:
```
Task:
    id = 10
    type = 'INFERNCE'
    data = np.ndarray
```

### Task generation
During the producers runtime it continuously creates new tasks and temporarily stores them in an internal buffer, called the task queue. 
The task queue operates on a First in First Out (FIFO) basis.
The tasks remain in this queue until a worker requests work via the [Work-API](#work-api), consuming the task.
If the tasks are produced faster than they are consumed, then the task queue will grow indefinitely. In order to mitigate this problem the producers acts as a type of [controlling entity](#controlling-entity). 
Meaning that it can manipulate certain parameters to increase/decrease the overall required computational load in order to mitigate the concerns of a growing task queue.

The generation and distribution of tasks are handled in different threads in order to operate in parallel.
### End of the stream
When the producer has finished it signals all workers to shut down by sending a `Task` with `type=END`. 
This will run through every node in the system and initiate a graceful shutdown of the pipeline.

### Elasticity
The producer also monitors and adjusts stream parameters to maintain user-defined QoE under constrained resources
It tracks information about the current state of the video stream and propagating changes when they occur.
The three video stream QoE parameters are:

1. **FPS:** FPS of the video stream
2. **Resolution:** Resolution of the video stream.
3. **Quality:** Grade of the YOLOv11 model used.

It aims to:
- Maximize these parameters
- While meeting certain [Service Level Objectives (SLOs)](#service-level-objectives-slos): Memory Usage, Task Queue Size

Under the constraints of the SLOs the producer aims to maximise the following 3 parameters (goals):
1. keep resolution as close as possible to the source-resolution of the video-stream
2. keep fps as close as possible to the source-fps of the underlying video-stream
3. maximize the result of the YOLOv11 inference (Maximize Inference Quality)
Keep in mind that higher stream parameters lead to a higher computational demand from the workers.

#### Video Stream Parameters
**Stream Parameter Adjustment:** If SLOs are at risk, the producer (via the AIF agent) may:

- **Inference Quality:** Switch to a different grade YOLOv11 model. (LOW, MEDIUM, HIGH)
- **FPS:** Change source stream FPS (int)
- **Resolution:** Change source stream resolution (Width & Height)

While the producer tries to fulfill the SLOs and maximize parameters (inference quality, fps, resolution) at the same time, it is crucial that the SLO are of a much higher priority compared to the parameters.
Especially since the parameters directly influence the probability of fulfilling the SLOs. Maximizing the video stream parameters is more a preference, rather than a priority

#### Changing of streaming parameters
The producer is able to directly control the streaming parameters. In this example these are Resolution, FPS, Inference Quality

##### Frames per Second (FPS)
The FPS is simply a parameter set at the producer level. The producer simply produces tasks in accordance to the number of fps set.
Meaning if our data-stream is 30FPS then the producer will generate 30 tasks per second (TPS). These tasks are generated at regular intervals.
So if we generate 30TPS, a task will be generated every 0,03333 seconds = 1/30.

##### Resolution
The resolution is simply are parameter set at the producer level. As each task contains the image, the producer resizes this image before creating the tasks.
Meaning if the best possible resolution is 1920x1080, but the resolution is set to 1280x720 then the producer will simply rescale the image when creating a task.

##### Inference Quality
The producer dictates the inference quality, i.e. the YOLOv11 model that is used to process the frames.
When the Inference Quality parameter changes at the producer level it is very important that the workers change their model as quickly as possible.
This is realised by a backlog that the producer stores for each worker in the system. So when a change occurs this change is stored in the backlog of all workers as a pending change.

When a worker node requests a task from the producer, the backlog for this node is checked.
If there is a pending change. For example a reduction in inference quality (HIGH -> MEDIUM) then worker receives a reply with type='CHANGE' detailing the following changes:
- CHANGE_INFERENCE_QUALITY: MEDIUM
indicating that a change of YOLOv11 model is necessary before continuing with stream processing.
The worker then makes this change locally and continues with regular requests for new work.

### Service Level Objectives (SLOs)
A Service Level Objective (SLO) is a measurable objective that a system can enforce. In this case we choose our SLOs in a way so that the producer can elastically adapt, by changing the 3 quality parameters (Resolution, FPS, Inference Quality)
The Service Level Objectives (SLOs) are implemented by the Producer in order to ensure the highest possible Quality of Experience (QoE) given the current available resources.
Possible SLOs include: processing time for x amount frames, energy consumption, buffer size, memory usage, etc.

SLOs guide the system to maintain QoE within operational limits. A SLO is implemented through a normalized ratio between the current observation $x$ and the associated threshold $\theta$. 
This ratio, hereafter referred to as the *SLO value*, is computed as
$$
\text{SLO Value} = \frac{x}{\theta}
$$.

An SLO value less than 1 indicates that the constraint is fulfilled. Conversely, values exceeding 1 (\(\geq1\)) signal an SLO violation, necessitating corrective action.
Currently only 2 SLOs are implemented:
- Memory Usage
- Queue Size

#### Representation in the code
The SLOs are calculated as float values with bound 0.0-infinity. The higher the value of the SLO, the worse its status. 
- ``0.0 <= slo-value < 0.9`` indicates a fulfilled SLO in a good state.
- ``0.9 <= slo-value < 1.0`` indicates a fulfilled SLO in a warning state. The SLO is fulfilled but an increase in computational demand might lead to an unfullfilled SLO.
- ``1.0 <= slo-value < MAX_FLOAT`` indicates an unfulfilled SLO in a critical state

#### Memory Usage SLO
$$
\text{memory_usage} \leq X%
$$

- $X%$ maximum percentage of acceptable memory use

GOAL: Ensure memory usage does not exceed capacity. Consuming the entire memory of the producer will cause a massive slowdown, as the new tasks will either be stored on a slower type of storage or discarded entirely.

#### Task queue (buffer) size SLO
$$
\text{items in task_queue} \leq \text{current_fps} * \theta
$$

- $\theta \geq 1$ tolerance

GOAL: Make sure there is enough compute power to handle tasks in real time. 
This SLO makes sure that the internal task buffer of the producer does not grow indefinitely. 
If the buffer remains below the chosen threshold, then this indicates that there is enough computational resources available on the worker side to handle the demand of the current video stream parameters.

#### Average Global Task Processing Time
Workers send their stats during their requests (e.g., the processing time of the last frame)
Producer tracks the average processing time of the last n frames, regardless of who processed them, using moving average. n defines the window size, e.g. 30

$$
\text{avg_processing_time} \leq \frac{1}{\text{current_fps}} \cdot \theta
$$
with $\text{current_fps}=30$ for a 30fps video. 
Usually we set $\theta = 1$, To make sure the average processing time per tasks is at least as fast as the source video stream.

GOAL: Make sure worker are processing tasks fast enough to keep up with the input.

#### Average Task Processing Time per Worker
For each worker, the Producer tracks the average processing time of the last n frames, using moving average.

For each Worker, the average processing time must satisfy:

$$
\text{avg_processing_time} \leq \frac{1}{\text{current_fps}} \cdot \theta
$$

where $\theta \geq 1$ represents additional tolerance within the frame budget $\frac{1}{\text{current\_fps}}$.
Usually we set $\theta \geq 2$, to allow slow nodes to take up to e.g., twice as long for computing a single task.

GOAL: Avoid a configuration where slower workers cause the system to drop tasks (frames).  
When a worker is below a certain threshold for processing a single \textbf{task}, then this tasks may not be used in the resulting output. 
This is because by the time the slow worker is done processing its tasks, the output-stream has already moved past this point.

HOW IT IS CALCULATED:
Take Worker with highest avg_processing_time. Calculate the value using:
$$
\text{SLO Value} = \frac{\text{max avg processing time}}{\frac{1}{\text{current_fps}} \cdot \theta}
$$.


### Producer as Controlling entity: 
It is a core methodology that only the producer decides and makes changes in the system.
Observations are gathered either directly at the producer level (Queue Size, Memory), or received from workers in ``GET_WORK`` requests.
These observations are then used to evaluated SLOs and in turn stream quality parameters

### Implementation of Active Inference
The agent that controls the elasticity of the system runs on the producer. It is implemented using the active inference library [pymdp](https://github.com/infer-actively/pymdp). More information is available in the offical [paper](https://arxiv.org/abs/2201.03904).
In order to make intelligent decisions using active inference, we need to model the problem as a Partially observable Markov decision process (POMDP). For this the environment was modeled as follows:

Observations:
- FPS of stream
- Resolution of stream
- YOLOv11 Inference Quality (Model) used by the worker nodes
- State of the Memory SLO (as a normalized float value)
- State of the Queue Size SLO (as a normalized float value)
- State of the Global Average Processing Time SLO (as a normalized float value)
- State of the Highest Worker Average Processing Time (as a normalized float value)

Actions set 1 (absolute control):
- Direct changing of the FPS of the stream
- Direct changing the Resolution of the stream
- Direct Changing the YOLOv1 Inference Quality (The model used by the worker nodes)

Actions set 2 (relative control):
- Increasing/Decreasing of the FPS of the stream by one step
- Increasing/Decreasing of the Resolution of the stream by one step
- Increasing/Decreasing of YOLOv1 Inference Quality (The model used by the worker nodes) by one step

#### Goal
1. Uphold all 4 SLOs at all times.
2. While upholding the SLOs: maximize the 3 stream quality parameters (fps, resolution, inference quality)
The preference for the stream quality parameters should be resolution > fps > inference quality, if the performance impact is equal.



#### AIF Loop
The agent defines an interval for the AIF loop. Per default this is 1 second.
Meaning every 1s:
- The agent retrieves the observations (SLO values and stream quality parameters)
- The agent updates its believes
- The agent chooses actions according to its believes

## Worker
The workers purpose is to process the tasks provided by the producer. It continuously
1. request work from the producer.
2. processes it by some form of computation (e.g.  YOLOv11 inference)
3. sends the result to the collector.

### Requesting Work
The worker requests work whenever its internal task-buffer (a queue containing TODO tasks ) is empty. The work requesting is done according to the [Work-API](#work-api).

### Processing the task
The worker takes tasks from the task-buffer and runs inference on them using YOLOv11. The result is then stored in the result-buffer (a queue containing processed tasks).

The computation is done in a separate process to enable maximum performance.
### Sending Results to Collector
The worker takes the results (processed tasks) from the result-buffer and sends them to the collector using a zeromq.PUSH`

#### Results
Results are identical to ``Task`` objects, with the only difference being that `type=COLLECT`

These 3 concerns (requesting, processing, sending) are implemented in their own threads/processes in order to achieve a non-blocking task processing pipeline.

## Collector
The collector continuously accepts results from  
The Collector implements a ``zeromq.PULL`` socket that constantly accepts results from workers and aggregates them to produce the final output video-stream.

## Work-API
Implemented by the Producer and used by the Worker.

The Producer implements a `zeromq.ROUTER` socket, that is waiting for requests and returning a task.
The Worker implements a `zeromq.REQ` socket, that is requesting tasks from the Producer's `zeromq.ROUTER` socket.

The communication is request-reply structure:
1. The Worker sends a `REQ`
2. The Producer replies with a `REP`

This is often referred to as the [Load Balancing Pattern](https://zguide.zeromq.org/docs/chapter3/#The-Load-Balancing-Pattern). 
Using this approach, the producer does not decide the distribution of work, rather the decision is made by the worker nodes, as they can choose when to work.
If the workers are configured to request tasks whenever possible, such a design maximises the resource utilization of the workers and therefore the entire distributed system, as each worker requests work up to its own maximum capacity. 
The downside of such an architecture is the added overhead, as the workers have to explicitly request work, whenever their internal buffer is empty.
This overhead is justified as it negligible in comparison to the large amounts of ``Task.data`` that sent via reply of the producer and enables proper load balancing of the worker nodes. 

### General Request-Reply Structure
`REQ:` A dict that defines the type of request and optional additional information.
```python
req = {
    type: str,
    #(optional additional information)
}
```

`REP:` multipart message containing
General information about the response:
```python
info = {
    type: str,
    #(optional additional information)
}
```
additionally for each task:
```python
task_metadata = {
    id: int,
    dtype: str,
    shape: tuple[int, int, int]
}
```
```python
data: numpy.ndarray
```

So for example a response containg a singular ``Task`` would look like this:
A multipart message containg
```python
info, task_metadata
``` 
```python
{type: str, #(optional additional information)}, {id, dtype, shape}, task
``` 
```python
{type=INFERENCE}, {id=0, dtype=int64, shape=(1920,1080,3)}, np.ndarray
``` 

### Regular REQ-REP
##### Register -> Confirmation
When a worker starts it sends a registration message to the producer in order to get the current stream configuration.
This makes sure that the worker does not need to be configured individually, solely relying on the provided configuration of the producer.

The worker then loads the required model(s), depending on the retrieved configuration, and adjusts the internal configuration. Upon completing the setup process, the worker starts with the work requesting process
`REQ:` 
```python
req = {
    type='REGISTER'
}
```

`REP:` 
```python
info = {
    type='REGISTRATION_CONFIRMATION',
    work_type:str ∈ ['NONE', 'YOLO_DETECTION', 'YOLO_OBB']
    work_type:str ∈ ['LOW', 'MEDIUM', 'HIGH']
    loading_mode:str ∈ [0 (LAZY), 1 (EAGER)]
}
```
##### Request Work -> Receive Work
General mode of operation.

`REQ:` 
```python
req = {
    type='GET_WORK'
}
```

`REP:` multipart message containing
```python
info = {
    type:str ∈ ['INFERENCE', (Other potential work types)]
}
```
additionally for each task:
```python
task_metadata = {
    id: int,
    dtype: str,
    shape: tuple[int, int, int]
}
```
```python
data: numpy.ndarray
```

##### Request Work -> Receive Change
This happens when there is a crucial change in the current parameter configuration of the stream. The producer replies with a list of changes instead of tasks. 
This is because it is of the utmost importance that the workers adjust to the changes before continuing processing new tasks.

`REQ:` 
```python
req = {
    type='GET_WORK'
}
```

`REP:`
```python
info = {
    type='CHANGE'
    (optional)  CHANGE_INFERENCE_QUALITY:int ∈ [0 (LOW), 1(MEDIUM), 2 (HIGH)]
    (optional) change-2=value2
    (optional) change-3=value2
    ...
}
```


For example if the work load for the YOLOv11 inference needs to be changed to 1 (MEDIUM) a ``REP`` would look like this:

`REP:`
```python
info = {
    type='CHANGE'
    CHANGE_INFERENCE_QUALITY=1
}
```

### Other Responses
#### Any Rquest -> END of transmission
This occurs when the producer is stopping and has no more tasks left. This signals the workers and by extension the collector to shut down.
```python
req = {
    type=ANY
}
```

`REP:`
```python
info = {
    type='END'
}
```


# Evaluation
In order to evaluate the effectiveness of the implementation we implement the following:

## Agent Types
We implement other types of agents that control the elasticity of the system. We implement two different agent types:
The Active Inference Agent (AIF). This is the main and most important agent of this project. 
In order to evaluate its effectiveness this prototype also implements one other more standards agent's for comparison:

### Heuristic Agent
In order to evaluate the effectiveness of statistical models, such as AIF, we implemented a simple agent based on heuristic measurements.

The agent defines an interval for the loop. Per default this is 1 second.
Meaning every 1s:
- The agent retrieves the values of the SLO
- The agent evaluates the values.
- If a SLO is in CRITICAL state (i.e. value > 1.0) the agent reduces the stream quality parameters
- If a SLO is in WARNING state, the agent does nothing.
- If all SLOs are in OK state, the agent tries to improve the stream quality parameters.

Both agents share the same goal of upholding the SLOs while trying to keep high QoE, through elasticity (Changing of quality parameters).

## Simulation
In order to properly evaluate the agent types we have set a simulation. The simulation consists of a simple streaming scenario.

### Environment
The entire simulation is running a single computer with the specs:
- **Operating System:** Windows 11, Version 23H2 (Build 22631.5335)
- **Python:** Python 3.12.2
- **CPU:** AMD Ryzen 7 7800X3D
- **GPU:** Nvidia GeForce GTX 1660Ti (MSI GTX Ti Ventus XS OC)
- **Memory:** 32GB DDR5 RAM, Clock: 4800MHz, Dual Channel, Timing: 40-40-40-77, tRC: 117, tRFC: 708
- **Drive:** WD Black SN770 2TB

### Setup
- 1 Producer
- n Workers (3 Workers typically)
- 1 Collector

The video stream generated by the producer is a video of highway traffic. The goal is to detect the vehicles passing by using YOLOv11 and drawing bounding boxes around them.
All workers have the same amount of processing power. 
n Workers start at the same time and register themselves at the producer. During the registration processes the receive the necessary configuration from the producer and load the necessary YOLO model.
When the workers configuration is fully set up they start requesting work at the Producer. This continues until the original video streams stops and the producer only answers request with ``END``.

In order to measure the elastic capabilities of the agents, m number (1 typically) of the agents will go offline temporarily causing the total amount of computational resources of the distributed system to decrease.
This will cause the values of the SLOs to change into a critical state. This will prompt to agent to take an action in order to achieve equilibrium.
This will be done by decreasing the quality parameters in some way, decreasing the computational requirements in such a way that the remaining workers are able to uphold the SLO.
After a certain period the agents will recover and come back online. This will cause the total total amount of computational resources of the distributed system to increase.
The agent should then notice this increasing in available resources and try to increase the stream quality parameters to facilitate a better QoE.
The outage and recovery of the workers are set at 25% and 75% of the total stream duration respectively.

During the entire runtime of the simulation the producer tracks certain metrics:
- Registration time of workers
- Number of requested tasks per worker
- Stream Quality Parameters Capacity (FPS, Resolution, Inference Quality) over time. The capacity is represented as a float value (0.0-1.0) Where 0 is the worst possible configuration and 1.0 is the best possible configuration.
- Queue Size over time (Number of Tasks in queue)
- Memory Usage % over time
- Queue Size SLO value over time
- Memory Usage SLO value over time
and return as a python dataframes.

These values are then plotted using [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/). 

### Evaluation Cases
3 Simulations Cases are implemented:
1. Base 
2. Variable Computational Budget
3. Variable Computational Load

#### Base Case
A regular 30fps video stream.
#### Variable Computational Budget Case
During 25% of the streams runtime a worker will go offline and re-join at 75% of the streams runtime. 
When the worker goes offline the computational Budget of the system decreases, below the required threshold to processes the stream in a timely fashion.
The Producer should react accordingly by reducing the stream quality parameters, when the worker goes offline and increasing them back to baseline when the worker re-joins.
#### Variable Computational Load Case
During 25% of the streams runtime, two "additional" video streams will be produced by the producer. At 75% runtime it returns to a single stream.
In order to simulate multiple streams, the frames of the underlying video stream are just multiplied. i.e., 
- for 1 stream: 1 task is generated
- for 2 streams: 2 tasks with equivalent frame data are generated
- for 3 streams: 3 tasks with equivalent frame data are generated

# Implementation
## Compute Platform
Nvidia [CUDA 12.8](https://developer.nvidia.com/cuda-12-8-0-download-archive) and the respective [pytorch](https://download.pytorch.org/whl/cu128) version
## Inference Model
Ultralytics [YOLOv11](https://docs.ultralytics.com/models/yolo11/)
## Network Communication 
python ZeroMQ implementation [pyzmq](https://zeromq.org/languages/python/). 
Supports effective sending of numpy arrays using this [functionality](https://pyzmq.readthedocs.io/en/latest/howto/serialization.html#example-numpy-arrays).
## Active Inference Library
[pymdp](https://github.com/infer-actively/pymdp)
## Causel Inference via DAG
[pgmpy](https://pgmpy.org/) used in in [intelligentVehicle](https://github.com/borissedlak/intelligentVehicle)
## Serialization
[msgpack](https://msgpack.org/index.html) because the result is slightly smaller than [pickle](https://docs.python.org/3/library/pickle.html) and has similar performance in serialization speed for this use case.
## Reinforcement Learning (RL) Library
[stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/)
## Plotting
[matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/)
## Date collection
[Pandas](https://pandas.pydata.org/)

# Dependencies
## How to build `requirements.txt`
```pip
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip install ultralytics
pip install msgpack
pip install pyzmq
pip install inferactively-pymdp
pip install stable-baselines3
pip install gym


pip freeze > requirements.txt
```

Add on top of requirements.txt
```
-f https://download.pytorch.org/whl/cu126/torch
-f https://download.pytorch.org/whl/cu126/torchvision
-f https://download.pytorch.org/whl/cu126/torchaudio
```




# TODO
Optional:
    When changing fps: notify the collector that the fps are changed
    Send multiple frames at once Producer -> Worker && Worker -> Collector

