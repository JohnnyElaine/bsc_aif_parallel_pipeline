# Description
This python project implements a distributed parallel pipeline for processing streaming data in edge computing environments.
The main idea is to split a data stream into smaller chunks that are distributed across multiple workers (edge nodes) running in parallel. 
Each worker node processes its chunk, and the results are aggregated by a collector node to form a modified output stream.

The architecture draws inspiration from a [Apache Storm Topology](https://storm.apache.org/releases/2.7.1/Tutorial.html) and [Apache Flink](https://flink.apache.org/), consisting of:
A single Sprout (The collector), an internal layer of Bolts (Workers) and an aggregate Bolt (Collector).


For evaluation, a video stream of a highway is used as input, with YOLOv11 running inference in order to detect vehicles on live video frames. 
However, the framework is generic and applicable to any parallelizable data stream, such as audio, LIDAR, RADAR, network traffic, or sensor data.
**Central Topic**
"How to use Active Inference to uphold Service Level Objectives in edge computing scenarios"
**Central Research Question:**
"How can i efficiently process data streams in resource limited edge computing cases"
This is done by:
1. Implementing a distributed pipeline for parallel processing of data.
2. Employing Service Level Objectives (SLOs) in order uphold quality standards and abide by resource constraints
3. Implement elasticity in order to dynamically scale certain stream paramters in order to control the computational demand.
4. implementing an active inference (AIF) based agent in order to control elasticity demands

## Motivation
Edge computing offers scalable, low-latency processing closer to data sources, but is constrained by limited resources. 
This pipeline supports elasticityâ€”the ability to dynamically adjust stream quality parameters in order to uphold a certain level of Quality of Experience (QoE) / Quality of Service (QoS) depending on the stream type, even when resources are constraint.
Elasticity specifically meaning to ability to dynamically manipulate certain quality parameters of the datastream in order to control overall computational load on the worker-nodes. 
Because we are in an edge computing scenario, we can't simply add more worker nodes when computational demand is high. 
This means that if the output stream of the parallel pipeline is not up to predefined QoE standard, then the stream must reduce quality parameters of some kind in order to reduce overall computational load on the system.

The decision making process regarding the elasticity is implemented using an Active Inference (AIF) based Agent. 
Active Inference (AIF) is an emerging concept from neuroscience that describes how the brain continuously predicts and evaluates sensory information to model real-world processes.
The goal if the AIF agent is to maximize the quality metrics of the output-stream while providing a predefined level of QoE. 
This delicate balance is implemented using certain preference regarding the quality metrics of the stream and Service Level Objectives (SLOs) in order to keep resource usage within bounds.

## Architecture
The pipeline consists of three main components:

- Producer: Generates and controls task flow.
- Worker: Processes tasks (e.g., YOLO inference).
- Collector: Aggregates processed results.

Throughout this README, examples refer to video streams and YOLOv11 inference, but the pipeline is general-purpose.

## Producer
The Producer continuously generates a stream of tasks. A ``Task`` object represents a single unit of work (e.g. a video frame).

### Task
A `Task` consists of 3 fields:
- `id` The `integer` id of the task
- `type` The `string` type of tasks. e.g. YOLOv11 inference
- `data` A `numpy.ndarray` containing a single frame of the video

#### Task Types
The worker nodes strictly adhere to the instructions stated within a given ``Task``. The general task types are:
- INFERENCE: Standard processing of frames using (YOLOv11) inference. Generated by the producer and carried out by the worker.
- COLLECT:  Standard collection of frames to re-assemble the data stream. Generated by the worker and carried out by the collector.
- CHANGE_INFERENCE_QUALITY: Change in inference quality. Switch (YOLOv11) model. Generated by the producer and carried out by the worker.
- END: End of the video stream. Propagate the END message and initiate graceful shutdown. Flow: Producer -> Worker -> Collector stopping all associated nodes in the process.

For example, when running inference on a video stream the producer would produce tasks such as:
```
Task:
    id = 10
    type = 'INFERNCE'
    data = np.ndarray
```

### Task generation
During the producers runtime it continuously creates new tasks and temporarily stores them in an internal buffer, called the task queue. 
The task queue operates on a First in First Out (FIFO) basis.
The tasks remain in this queue until a worker requests work via the [Work-API](#work-api), consuming the task.
If the tasks are produced faster than they are consumed, then the task queue will grow indefinitely. In order to mitigate this problem the producers acts as a type of [controlling entity](#controlling-entity). 
Meaning that it can manipulate certain parameters to increase/decrease the overall required computational load in order to mitigate the concerns of a growing task queue.

The generation and distribution of tasks are handled in different threads in order to operate in parallel.
### End of the stream
When the producer has finished it signals all workers to shut down by sending a `Task` with `type=END`. 
This will run through every node in the system and initiate a graceful shutdown of the pipeline.

### Elasticity
The producer also monitors and adjusts stream parameters to maintain user-defined QoE under constrained resources
It tracks information about the current state of the video stream and propagating changes when they occur.
The three QoE parameters are:

1. **FPS:** FPS of the video stream
2. **Resolution:** Resolution of the video stream.
3. **Quality:** Grade of the YOLOv11 model used.

It aims to:
- Maximize these parameters
- While meeting certain [Service Level Objectives (SLOs)](#service-level-objectives-slos): Memory Usage, Task Queue Size

Under the constraints of the SLOs the producer aims to maximise the following 3 parameters (goals):
1. keep resolution as close as possible to the source-resolution of the video-stream
2. keep fps as close as possible to the source-fps of the underlying video-stream
3. maximize the result of the YOLOv11 inference (Maximize Inference Quality)
Keep in mind that higher stream parameters lead to a higher computational demand from the workers.

#### Video Stream Parameters
**Stream Parameter Adjustment:** If SLOs are at risk, the producer (via the AIF agent) may:

- **Inference Quality:** Switch to a different grade YOLOv11 model. (LOW, MEDIUM, HIGH)
- **FPS:** Change source stream FPS (int)
- **Resolution:** Change source stream resolution (Width & Height)

While the producer tries to fulfill the SLOs and maximize parameters (inference quality, fps, resolution) at the same time, it is crucial that the SLO are of a much higher priority compared to the parameters.
Especially since the parameters directly influence the probability of fulfilling the SLOs. Maximizing the video stream parameters is more a preference, rather than a priority

#### Changing of streaming parameters
The producer is able to directly control the streaming parameters. In this example these are Resolution, FPS, Inference Quality

##### Frames per Second (FPS)
The FPS is simply a parameter set at the producer level. The producer simply produces tasks in accordance to the number of fps set.
Meaning if our data-stream is 30FPS then the producer will generate 30 tasks per second (TPS). These tasks are generated at regular intervals.
So if we generate 30TPS, a task will be generated every 0,03333 seconds = 1/30.

##### Resolution
The resolution is simply are parameter set at the producer level. As each task contains the image, the producer resizes this image before creating the tasks.
Meaning if the best possible resolution is 1920x1080, but the resolution is set to 1280x720 then the producer will simply rescale the image when creating a task.

##### Inference Quality
The producer dictates the inference quality, i.e. the YOLOv11 model that is used to process the frames.
When the Inference Quality parameter changes at the producer level it is very important that the workers change their model as quickly as possible.
This is realised by a backlog that the producer stores for each worker in the system. So when a change occurs this change is stored in the backlog of all workers as a pending change.

When a worker node requests a task from the producer, the backlog for this node is checked.
If there is a pending change. For example a reduction in inference quality (HIGH -> MEDIUM) then worker receives a task with type='CHANGE_INFERENCE_QUALITY' indicating that a change of YOLOv11 model is necessary before continuing with stream processing.
The worker then makes this change locally and continues with regular requests for new work.

### Service Level Objectives (SLOs)
A Service Level Objective (SLO) is a measurable objective that a system can enforce. In this case we choose our SLOs in a way so that the producer can elastically adapt, by changing the 3 quality parameters (Resolution, FPS, Inference Quality)
The Service Level Objectives (SLOs) are implemented by the Producer in order to ensure the highest possible Quality of Experience (QoE) given the current available resources.
Possible SLOs include: processing time for x amount frames, energy consumption, buffer size, memory usage, etc.

SLOs guide the system to maintain QoE within operational limits. A SLO is implemented through a normalized ratio between the current observation $x$ and the associated threshold $\theta$. 
This ratio, hereafter referred to as the *SLO value*, is computed as

SLO value = x/Theta
$$
\text{SLO Value} = \frac{x}{\theta}
$$.

An SLO value less than 1 indicates that the constraint is fulfilled. Conversely, values exceeding 1 (\(\geq1\)) signal an SLO violation, necessitating corrective action.
Currently only 2 SLOs are implemented:
- Memory Usage
- Queue Size

#### Representation in the code
The SLOs are calculated as float values with bound 0.0-infinity. The higher the value of the SLO, the worse its status. 
- ``0.0 <= slo-value < 0.9`` indicates a fulfilled SLO in a good state.
- ``0.9 <= slo-value < 1.0`` indicates a fulfilled SLO in a warning state. The SLO is fulfilled but an increase in computational demand might lead to an unfullfilled SLO.
- ``1.0 <= slo-value < MAX_FLOAT`` indicates an unfulfilled SLO in a critical state

#### Memory Usage SLO
```
memory_usage <= X%
```
- `X%` maximum percentage of acceptable memory use

GOAL: Ensure memory usage does not exceed capacity. Consuming the entire memory of the producer will cause a massive slowdown, as the new tasks will either be stored on a slower type of storage or discarded entirely.

#### Task queue (buffer) size SLO
```
task_queue <= X
```
- ``X`` maximum acceptable number of tasks. e.g. ``X = source_fps * 2``

GOAL: Make sure there is enough compute power to handle tasks in real time. This SLO makes sure that the internal task buffer of the producer does not grow indefinitely. If the buffer remains below the chosen threshold, then this indicates that there is enough computational resources available on the worker side to handle the demand of the current video stream parameters.

## Worker
The workers purpose is to process the tasks provided by the producer. It continuously
1. request work from the producer.
2. processes it by some form of computation (e.g.  YOLOv11 inference)
3. sends the result to the collector.

### Requesting Work
The worker requests work whenever its internal task-buffer (a queue containing TODO tasks ) is empty. The work requesting is done according to the [Work-API](#work-api).

### Processing the task
The worker takes tasks from the task-buffer and runs inference on them using YOLOv11. The result is then stored in the result-buffer (a queue containing processed tasks).

The computation is done in a separate process to enable maximum performance.
### Sending Results to Collector
The worker takes the results (processed tasks) from the result-buffer and sends them to the collector using a zeromq.PUSH`

#### Results
Results are identical to ``Task`` objects, with the only difference being that `type=COLLECT`

These 3 concerns (requesting, processing, sending) are implemented in their own threads/processes in order to achieve a non-blocking task processing pipeline.

## Collector
The collector continuously accepts results from  
The Collector implements a ``zeromq.PULL`` socket that constantly accepts results from workers and aggregates them to produce the final output video-stream.

## Work-API
Implemented by the Producer and used by the Worker.

The Producer implements a `zeromq.ROUTER` socket, that is waiting for requests and returning a task.
The Worker implements a `zeromq.REQ` socket, that is requesting tasks from the Producer's `zeromq.ROUTER` socket.

The communication is request-reply structure:
1. The Worker sends a `REQ`
2. The Producer replies with a `REP`

This is often referred to as the [Load Balancing Pattern](https://zguide.zeromq.org/docs/chapter3/#The-Load-Balancing-Pattern). 
Using this approach, the producer does not decide the distribution of work, rather the decision is made by the worker nodes, as they can choose when to work.
If the workers are configured to request tasks whenever possible, such a design maximises the resource utilization of the workers and therefore the entire distributed system, as each worker requests work up to its own maximum capacity. 
The downside of such an architecture is the added overhead, as the workers have to explicitly request work, whenever their internal buffer is empty.
This overhead is justified as it negligible in comparison to the large amounts of ``Task.data`` that sent via reply of the producer and enables proper load balancing of the worker nodes. 

### General Request-Reply Structure
`REQ:` A dict that defines the type of request and optional additional information.
```python
req = {
    type: str,
    #(optional additional information)
}
```

`REP:` multipart message containing
General information about the response:
```python
info = {
    type: str,
    #(optional additional information)
}
```
additionally for each task:
```python
task_metadata = {
    id: int,
    dtype: str,
    shape: tuple[int, int, int]
}
```
```python
data: numpy.ndarray
```

So for example a response containg a singular ``Task`` would look like this:
A multipart message containg
```python
info, task_metadata
``` 
```python
{type: str, #(optional additional information)}, {id, dtype, shape}, task
``` 
```python
{type=INFERENCE}, {id=0, dtype=int64, shape=(1920,1080,3)}, np.ndarray
``` 

### Regular REQ-REP
##### Register -> Confirmation
When a worker starts it sends a registration message to the producer in order to get the current stream configuration.
This makes sure that the worker does not need to be configured individually, solely relying on the provided configuration of the producer.

The worker then loads the required model(s), depending on the retrieved configuration, and adjusts the internal configuration. Upon completing the setup process, the worker starts with the work requesting process
`REQ:` 
```python
req = {
    type='REGISTER'
}
```

`REP:` 
```python
info = {
    type='REGISTRATION_CONFIRMATION',
    work_type:str âˆˆ ['NONE', 'YOLO_DETECTION', 'YOLO_OBB']
    work_type:str âˆˆ ['LOW', 'MEDIUM', 'HIGH']
    loading_mode:str âˆˆ [0 (LAZY), 1 (EAGER)]
}
```
##### Request Work -> Receive Work
General mode of operation.

`REQ:` 
```python
req = {
    type='GET_WORK'
}
```

`REP:` multipart message containing
```python
info = {
    type:str âˆˆ ['INFERENCE', (Other potential work types)]
}
```
additionally for each task:
```python
task_metadata = {
    id: int,
    dtype: str,
    shape: tuple[int, int, int]
}
```
```python
data: numpy.ndarray
```

##### Request Work -> Receive Change
This happens when there is a crucial change in the current parameter configuration of the stream. The producer replies with a list of changes instead of tasks. 
This is because it is of the utmost importance that the workers adjust to the changes before continuing processing new tasks.

`REQ:` 
```python
req = {
    type='GET_WORK'
}
```

`REP:`
```python
info = {
    type='CHANGE'
    (optional)  CHANGE_INFERENCE_QUALITY:int âˆˆ [0 (LOW), 1(MEDIUM), 2 (HIGH)]
    (optional) change-2=value2
    (optional) change-3=value2
    ...
}
```


For example if the work load for the YOLOv11 inference needs to be changed to 1 (MEDIUM) a ``REP`` would look like this:

`REP:`
```python
info = {
    type='CHANGE'
    CHANGE_INFERENCE_QUALITY=1
}
```

### Other Responses
#### Any Rquest -> END of transmission
This occurs when the producer is stopping and has no more tasks left. This signals the workers and by extension the collector to shut down.
```python
req = {
    type=ANY
}
```

`REP:`
```python
info = {
    type='END'
}
```

## Implementation of Active Inference
The agent that controls the elasticity of the system runs on the producer. It is implemented using the active inference library [pymdp](https://github.com/infer-actively/pymdp). More information is available in the offical [paper](https://arxiv.org/abs/2201.03904).
In order to make intelligent decisions using active inference, we need to model the problem as a Partially observable Markov decision process (POMDP). For this the environment was modeled as follows:

Observations:
- FPS of stream
- Resolution of stream
- YOLOv11 Inference Quality (Model) used by the worker nodes
- State of the Memory SLO (as a float value)
- State of the Queue Size SLO (as a float value)

Actions:
- Changing of the FPS of the stream
- Changing the Resolution of the stream
- Changing the YOLOv1 Inference Quality (The model used by the worker nodes)

### AIF Loop
The agent defines an interval for the AIF loop. Per default this is 1 second.
Meaning every 1s:
- The agent retrieves the observations (SLO values and stream quality parameters)
- The agent updates its believes
- The agent chooses actions according to its believes

## Evaluation
In order to evaluate the effectiveness of the implementation we implement the following:

### Additional Agents
We implement other types of agents that control the elasticity of the system. We implement two different agent types:
- Active Inference Agent (AIF), The one described above, i.e. the main one of this paper
- Heuristic Agent
- Reinforcement Learning (RL) Agent

All agents share the same goal of upholding the SLOs while trying to keep high QoE, through elasticity (Changing of quality parameters).

#### Reinforcement Learning Agent
This agent serves as a direct comparison to the active inference agent. This is because Reinforcement Learning (RL) is also framework for modeling intelligent behavior, but with a different fundamental underlying principle.
The agent is implemented using [stable-baselines](https://github.com/Stable-Baselines-Team/stable-baselines) and [Gymnasium (gym)](https://github.com/Farama-Foundation/Gymnasium).

#### Heuristic Agent
In order to evaluate the effectiveness of statistical models, such as AIF or RL, we implemented a simple agent based on heuristic measurements.

The agent defines an interval for the loop. Per default this is 1 second.
Meaning every 1s:
- The agent retrieves the values of the SLO
- The agent evaluates the values.
- If a SLO is in CRITICAL state (i.e. value > 1.0) the agent reduces the stream quality parameters
- If a SLO is in WARNING state, the agent does nothing.
- If all SLOs are in OK state, the agent tries to improve the stream quality parameters.

### Simulation
In order to properly evaluate the agent types we have set a simulation. The simulation consists of a simple streaming scenario.

#### Environment
The entire simulation is running a single computer with the specs:
- **Operating System:** Windows 11, Version 23H2 (Build 22631.5335)
- **Python:** Python 3.12.2
- **CPU:** AMD Ryzen 7 7800X3D
- **GPU:** Nvidia GeForce GTX 1660Ti (MSI GTX Ti Ventus XS OC)
- **Memory:** 32GB DDR5 RAM, Clock: 4800MHz, Dual Channel, Timing: 40-40-40-77, tRC: 117, tRFC: 708
- **Drive:** WD Black SN770 2TB

#### Setup
- 1 Producer
- n Workers (3 Workers typically)
- 1 Collector

The video stream generated by the producer is a video of highway traffic. The goal is to detect the vehicles passing by using YOLOv11 and drawing bounding boxes around them.
All workers have the same amount of processing power.  

In order to measure the elastic capabilities of the agents, m number (1 typically) of the agents will go offline temporarily causing the total amount of computational resources of the distributed system to decrease.
This will cause the values of the SLOs to change into a critical state. This will prompt to agent to take an action in order to achieve equilibrium.
This will be done by decreasing the quality parameters in some way, decreasing the computational requirements in such a way that the remaining workers are able to uphold the SLO.
After a certain period the agents will recover and come back online. This will cause the total total amount of computational resources of the distributed system to increase.
The agent should then notice this increasing in available resources and try to increase the stream quality parameters to facilitate a better QoE.
The outage and recovery of the workers are set at 25% and 75% of the total stream duration respectively.

During the entire runtime of the simulation the producer tracks certain metrics:
- Registration time of workers
- Number of requested tasks per worker
- Stream Quality Parameters Capacity (FPS, Resolution, Inference Quality) over time. The capacity is represented as a float value (0.0-1.0) Where 0 is the worst possible configuration and 1.0 is the best possible configuration.
- Queue Size over time (Number of Tasks in queue)
- Memory Usage % over time
- Queue Size SLO value over time
- Memory Usage SLO value over time
and return as a python dataframes.

These values are then plotted using [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/). 

# Implementation
## Compute Platform
Nvidia [CUDA 12.8](https://developer.nvidia.com/cuda-12-8-0-download-archive) and the respective [pytorch](https://download.pytorch.org/whl/cu128) version
## Inference Model
Ultralytics [YOLOv11](https://docs.ultralytics.com/models/yolo11/)
## Network Communication 
python ZeroMQ implementation [pyzmq](https://zeromq.org/languages/python/). 
Supports effective sending of numpy arrays using this [functionality](https://pyzmq.readthedocs.io/en/latest/howto/serialization.html#example-numpy-arrays).
## Active Inference Library
[pymdp](https://github.com/infer-actively/pymdp)
## Causel Inference via DAG
[pgmpy](https://pgmpy.org/) used in in [intelligentVehicle](https://github.com/borissedlak/intelligentVehicle)
## Serialization
[msgpack](https://msgpack.org/index.html) because the result is slightly smaller than [pickle](https://docs.python.org/3/library/pickle.html) and has similar performance in serialization speed for this use case.
## Reinforcement Learning (RL) Library
[stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/)
## Plotting
[matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/)
## Date collection
[Pandas](https://pandas.pydata.org/)

# Dependencies
## How to build `requirements.txt`
```pip
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip install ultralytics
pip install msgpack
pip install pyzmq
pip install inferactively-pymdp
pip install stable-baselines3
pip install gym


pip freeze > requirements.txt
```

Add on top of requirements.txt
```
-f https://download.pytorch.org/whl/cu126/torch
-f https://download.pytorch.org/whl/cu126/torchvision
-f https://download.pytorch.org/whl/cu126/torchaudio
```




# TODO
Optional:
    When changing fps: notify the collector that the fps are changed
    Send multiple frames at once Producer -> Worker && Worker -> Collector

