# Description
This python project implements a distributed parallel pipeline for processing streaming data in edge computing environments.
The main idea is to split a data stream into smaller chunks that are distributed across multiple workers (edge nodes) running in parallel. 
Each worker node processes its chunk, and the results are aggregated by a collector node to form a modified output stream.

The architecture draws inspiration from a [Apache Storm Topology](https://storm.apache.org/releases/2.7.1/Tutorial.html) and [Apache Flink](https://flink.apache.org/), consisting of:
A single Sprout (The collector), an internal layer of Bolts (Workers) and an aggregate Bolt (Collector).


For evaluation, a video stream of a highway is used as input, with YOLOv11 running inference in order to detect vehicles on live video frames. 
However, the framework is generic and applicable to any parallelizable data stream, such as audio, LIDAR, RADAR, network traffic, or sensor data.
**Central Topic**
"How to use Active Inference to uphold Service Level Objectives in edge computing scenarios"
**Central Research Question:**
"How can i efficiently process data streams in resource limited edge computing cases"
This is done by:
1. Implementing a distributed pipeline for parallel processing of data.
2. Employing Service Level Objectives (SLOs) in order uphold quality standards and abide by resource constraints
3. Implement elasticity in order to dynamically scale certain stream paramters in order to control the computational demand.
4. implementing an active inference (AIF) based agent in order to control elasticity demands

## Motivation
Edge computing offers scalable, low-latency processing closer to data sources, but is constrained by limited resources. 
This pipeline supports elasticity—the ability to dynamically adjust stream quality parameters in order to uphold a certain level of Quality of Experience (QoE) / Quality of Service (QoS) depending on the stream type, even when resources are constraint.
Elasticity specifically meaning to ability to dynamically manipulate certain quality parameters of the datastream in order to control overall computational load on the worker-nodes. 
Because we are in an edge computing scenario, we can't simply add more worker nodes when computational demand is high. 
This means that if the output stream of the parallel pipeline is not up to predefined QoE standard, then the stream must reduce quality parameters of some kind in order to reduce overall computational load on the system.

The decision making process regarding the elasticity is implemented using an Active Inference (AIF) based Agent. 
Active Inference (AIF) is an emerging concept from neuroscience that describes how the brain continuously predicts and evaluates sensory information to model real-world processes.
The goal if the AIF agent is to maximize the quality metrics of the output-stream while providing a predefined level of QoE. 
This delicate balance is implemented using certain preference regarding the quality metrics of the stream and Service Level Objectives (SLOs) in order to keep resource usage within bounds.

## Architecture
The pipeline consists of three main components:

- Producer: Generates and controls task flow.
- Worker: Processes tasks (e.g., YOLO inference).
- Collector: Aggregates processed results.

Throughout this README, examples refer to video streams and YOLOv11 inference, but the pipeline is general-purpose.

## Producer
The Producer continuously generates a stream of tasks. A ``Task`` object represents a single unit of work (e.g. a video frame).

### Task
A `Task` consists of 4 fields:
- `type` The `string` type of task (e.g., YOLOv11 inference, collection, changes)
- `id` The `integer` id of the task (used for ordering and tracking)
- `stream_key` The `integer` identifier to differentiate tasks between multiple concurrent streams
- `data` A `numpy.ndarray` containing a single frame of the video

The `stream_key` field is essential for handling multiple concurrent video streams in the Variable Computational Demand simulation scenarios. When the system processes multiple streams simultaneously (via stream multiplier), each stream is assigned a unique identifier to ensure proper task routing and result aggregation.

#### Task Types
The worker nodes strictly adhere to the instructions stated within a given ``Task``. The general task types are:
- INFERENCE: Standard processing of frames using (YOLOv11) inference. Generated by the producer and carried out by the worker.
- COLLECT:  Standard collection of frames to re-assemble the data stream. Generated by the worker and carried out by the collector.
- CHANGE_INFERENCE_QUALITY: Change in inference quality. Switch (YOLOv11) model. Generated by the producer and carried out by the worker.
- END: End of the video stream. Propagate the END message and initiate graceful shutdown. Flow: Producer -> Worker -> Collector stopping all associated nodes in the process.

For example, when running inference on a video stream the producer would produce tasks such as:
```
Task:
    type = 'INFERENCE'
    id = 10
    stream_key = 0
    data = np.ndarray
```

For multiple concurrent streams, tasks would have different stream keys:
```
# Stream 1 task
Task:
    type = 'INFERENCE'
    id = 10
    stream_key = 0
    data = np.ndarray

# Stream 2 task (different stream)
Task:
    type = 'INFERENCE'
    id = 10
    stream_key = 1
    data = np.ndarray
```

### Task generation
During the producers runtime it continuously creates new tasks and temporarily stores them in an internal buffer, called the task queue. 
The task queue operates on a First in First Out (FIFO) basis.
The tasks remain in this queue until a worker requests work via the [Work-API](#work-api), consuming the task.
If the tasks are produced faster than they are consumed, then the task queue will grow indefinitely. In order to mitigate this problem the producers acts as a type of [controlling entity](#controlling-entity). 
Meaning that it can manipulate certain parameters to increase/decrease the overall required computational load in order to mitigate the concerns of a growing task queue.

The generation and distribution of tasks are handled in different threads in order to operate in parallel.
### End of the stream
When the producer has finished it signals all workers to shut down by sending a `Task` with `type=END`. 
This will run through every node in the system and initiate a graceful shutdown of the pipeline.

### Elasticity
The producer also monitors and adjusts stream parameters to maintain user-defined QoE under constrained resources
It tracks information about the current state of the video stream and propagating changes when they occur.
The three video stream QoE parameters are:

1. **FPS:** FPS of the video stream
2. **Resolution:** Resolution of the video stream.
3. **Quality:** Grade of the YOLOv11 model used.

It aims to:
- Maximize these parameters
- While meeting certain [Service Level Objectives (SLOs)](#service-level-objectives-slos): Memory Usage, Task Queue Size

Under the constraints of the SLOs the producer aims to maximise the following 3 parameters (goals):
1. keep resolution as close as possible to the source-resolution of the video-stream
2. keep fps as close as possible to the source-fps of the underlying video-stream
3. maximize the result of the YOLOv11 inference (Maximize Inference Quality)
Keep in mind that higher stream parameters lead to a higher computational demand from the workers.

#### Video Stream Parameters
**Stream Parameter Adjustment:** If SLOs are at risk, the producer (via the AIF agent) may:

- **Inference Quality:** Switch to a different grade YOLOv11 model. (LOW, MEDIUM, HIGH)
- **FPS:** Change source stream FPS (int)
- **Resolution:** Change source stream resolution (Width & Height)

While the producer tries to fulfill the SLOs and maximize parameters (inference quality, fps, resolution) at the same time, it is crucial that the SLO are of a much higher priority compared to the parameters.
Especially since the parameters directly influence the probability of fulfilling the SLOs. Maximizing the video stream parameters is more a preference, rather than a priority

#### Changing of streaming parameters
The producer is able to directly control the streaming parameters. In this example these are Resolution, FPS, Inference Quality

##### Frames per Second (FPS)
The FPS is simply a parameter set at the producer level. The producer simply produces tasks in accordance to the number of fps set.
Meaning if our data-stream is 30FPS then the producer will generate 30 tasks per second (TPS). These tasks are generated at regular intervals.
So if we generate 30TPS, a task will be generated every 0,03333 seconds = 1/30.

##### Resolution
The resolution is simply are parameter set at the producer level. As each task contains the image, the producer resizes this image before creating the tasks.
Meaning if the best possible resolution is 1920x1080, but the resolution is set to 1280x720 then the producer will simply rescale the image when creating a task.

##### Inference Quality
The producer dictates the inference quality, i.e. the YOLOv11 model that is used to process the frames.
When the Inference Quality parameter changes at the producer level it is very important that the workers change their model as quickly as possible.
This is realised by a backlog that the producer stores for each worker in the system. So when a change occurs this change is stored in the backlog of all workers as a pending change.

When a worker node requests a task from the producer, the backlog for this node is checked.
If there is a pending change. For example a reduction in inference quality (HIGH -> MEDIUM) then worker receives a reply with type='CHANGE' detailing the following changes:
- CHANGE_INFERENCE_QUALITY: MEDIUM
indicating that a change of YOLOv11 model is necessary before continuing with stream processing.
The worker then makes this change locally and continues with regular requests for new work.

### Service Level Objectives (SLOs)
A Service Level Objective (SLO) is a measurable objective that a system can enforce. In this case we choose our SLOs in a way so that the producer can elastically adapt, by changing the 3 quality parameters (Resolution, FPS, Inference Quality)
The Service Level Objectives (SLOs) are implemented by the Producer in order to ensure the highest possible Quality of Experience (QoE) given the current available resources.
Possible SLOs include: processing time for x amount frames, energy consumption, buffer size, memory usage, etc.

SLOs guide the system to maintain QoE within operational limits. A SLO is implemented through a normalized ratio between the current observation $x$ and the associated threshold $\theta$. 
This ratio, hereafter referred to as the *SLO value*, is computed as
$$
\text{SLO Value} = \frac{x}{\theta}
$$.

An SLO value less than 1 indicates that the constraint is fulfilled. Conversely, values exceeding 1 (\(\geq1\)) signal an SLO violation, necessitating corrective action.
Currently only 2 SLOs are implemented:
- Memory Usage
- Queue Size

#### Representation in the code
The SLOs are calculated as float values with bound 0.0-infinity. The higher the value of the SLO, the worse its status. 
- ``0.0 <= slo-value < 0.9`` indicates a fulfilled SLO in a good state.
- ``0.9 <= slo-value < 1.0`` indicates a fulfilled SLO in a warning state. The SLO is fulfilled but an increase in computational demand might lead to an unfullfilled SLO.
- ``1.0 <= slo-value < MAX_FLOAT`` indicates an unfulfilled SLO in a critical state

#### Memory Usage SLO
$$
\text{memory_usage} \leq X%
$$

- $X%$ maximum percentage of acceptable memory use

GOAL: Ensure memory usage does not exceed capacity. Consuming the entire memory of the producer will cause a massive slowdown, as the new tasks will either be stored on a slower type of storage or discarded entirely.

#### Task queue (buffer) size SLO
$$
\text{items in task_queue} \leq \text{current_fps} * \theta
$$

- $\theta \geq 1$ tolerance

GOAL: Make sure there is enough compute power to handle tasks in real time. 
This SLO makes sure that the internal task buffer of the producer does not grow indefinitely. 
If the buffer remains below the chosen threshold, then this indicates that there is enough computational resources available on the worker side to handle the demand of the current video stream parameters.

#### Average Global Task Processing Time
Workers send their stats during their requests (e.g., the processing time of the last frame)
Producer tracks the average processing time of the last n frames, regardless of who processed them, using moving average. n defines the window size, e.g. 30

$$
\text{avg_processing_time} \leq \frac{1}{\text{current_fps}} \cdot \theta
$$
with $\text{current_fps}=30$ for a 30fps video. 
Usually we set $\theta = 1$, To make sure the average processing time per tasks is at least as fast as the source video stream.

GOAL: Make sure worker are processing tasks fast enough to keep up with the input.

#### Average Task Processing Time per Worker
For each worker, the Producer tracks the average processing time of the last n frames, using moving average.

For each Worker, the average processing time must satisfy:

$$
\text{avg_processing_time} \leq \frac{1}{\text{current_fps}} \cdot \theta
$$

where $\theta \geq 1$ represents additional tolerance within the frame budget $\frac{1}{\text{current\_fps}}$.
Usually we set $\theta \geq 2$, to allow slow nodes to take up to e.g., twice as long for computing a single task.

GOAL: Avoid a configuration where slower workers cause the system to drop tasks (frames).  
When a worker is below a certain threshold for processing a single \textbf{task}, then this tasks may not be used in the resulting output. 
This is because by the time the slow worker is done processing its tasks, the output-stream has already moved past this point.

HOW IT IS CALCULATED:
Take Worker with highest avg_processing_time. Calculate the value using:
$$
\text{SLO Value} = \frac{\text{max avg processing time}}{\frac{1}{\text{current_fps}} \cdot \theta}
$$.


### Producer as Controlling entity: 
It is a core methodology that only the producer decides and makes changes in the system.
Observations are gathered either directly at the producer level (Queue Size, Memory), or received from workers in ``GET_WORK`` requests.
These observations are then used to evaluated SLOs and in turn stream quality parameters

### Active Inference Agent Implementation
The Active Inference Agent controls the elasticity of the system and runs on the producer. It is implemented using the active inference library [pymdp](https://github.com/infer-actively/pymdp), which provides a comprehensive framework for building agents based on the Free Energy Principle. More information about the underlying theory is available in the official [paper](https://arxiv.org/abs/2201.03904).

Active Inference is a theoretical framework from computational neuroscience that describes how biological agents (like the brain) make decisions under uncertainty. The core idea is that agents maintain internal models of their environment and continuously update these models based on sensory observations. They then use these models to predict future states and select actions that minimize "free energy" - a quantity that combines prediction error with the complexity of the agent's beliefs.

#### POMDP Modeling
To make intelligent decisions using active inference, the elasticity control problem is modeled as a Partially Observable Markov Decision Process (POMDP). The agent interfaces with the distributed system through observations and actions:

**Observations (7 dimensions):**
- Current FPS of the stream (discrete state index)
- Current resolution of the stream (discrete state index)  
- Current YOLOv11 inference quality/model used by worker nodes (discrete state index)
- Queue Size SLO status (OK, WARNING, CRITICAL)
- Memory Usage SLO status (OK, WARNING, CRITICAL)
- Global Average Processing Time SLO status (OK, WARNING, CRITICAL)
- Worker Average Processing Time SLO status (OK, WARNING, CRITICAL)

**Actions (Relative Control Approach):**
The agent uses relative control, meaning it can incrementally adjust parameters:
- Increase/Decrease/No-change FPS of the stream by one step
- Increase/Decrease/No-change Resolution of the stream by one step  
- Increase/Decrease/No-change YOLOv11 Inference Quality by one step

#### Generative Model Components
The Active Inference agent learns through four key matrices that define its generative model:

1. **A Matrix (Observation Model)**: Maps from hidden states (stream parameters) to observations. Encodes the agent's beliefs about how computational load affects SLO violations.

2. **B Matrix (Transition Model)**: Defines state transitions based on actions. For this domain, transitions are largely deterministic since the agent can reliably change stream parameters.

3. **C Matrix (Preference Model)**: Encodes the agent's preferences over observations. Higher quality parameters are preferred, but SLO satisfaction takes priority over quality optimization.

4. **D Matrix (Prior Beliefs)**: Initial beliefs about the current state, typically initialized based on the actual current system configuration.

#### Learning and Adaptation
The agent continuously learns and adapts its internal model through:
- **A-matrix updates**: Learning how computational load correlates with SLO violations based on observed outcomes
- **Belief updates**: Updating beliefs about the current hidden state based on new observations
- **Policy inference**: Selecting action sequences that minimize expected free energy

#### Goals and Objectives
1. **Primary Goal**: Uphold all 4 SLOs at all times (Queue Size, Memory Usage, Global Processing Time, Worker Processing Time)
2. **Secondary Goal**: While maintaining SLO compliance, maximize the 3 stream quality parameters 
3. **Learning Goal**: Adapt and improve decision-making over time based on system feedback

The preference hierarchy for quality parameters is: **resolution > fps > inference quality** when performance impact is equivalent.

#### AIF Decision Loop
The agent operates on a configurable interval (default: 1 second):
1. **Observe**: Retrieve current SLO values and stream quality parameters
2. **Infer States**: Update beliefs about hidden states using Bayesian inference
3. **Infer Policies**: Evaluate potential action sequences and their expected outcomes
4. **Act**: Sample and execute actions based on policy that minimizes expected free energy
5. **Learn**: Update the generative model based on observed outcomes

## Worker
The worker's purpose is to process tasks provided by the producer. Each worker implements a sophisticated multi-process, multi-threaded architecture designed for high-performance, non-blocking task processing in edge computing environments.

### Worker Architecture Overview
Each worker operates as a separate Python process and internally manages three specialized sub-processes connected via inter-process communication pipes. This design maximizes parallelism and ensures that network I/O, computation, and result transmission can occur simultaneously without blocking each other.

#### Process-Level Architecture
The worker spawns **three separate processes** that operate concurrently:

1. **Work Requesting Pipeline** (Process)
2. **Task Processing Pipeline** (Process) 
3. **Result Sending Pipeline** (Process)

#### Inter-Process Communication (IPC)
The processes communicate through **Python multiprocessing Pipes** and **shared memory Values**:

**Communication Channels**:
- **Task Pipe**: Unidirectional pipe from Work Requesting → Task Processing
- **Result Pipe**: Unidirectional pipe from Task Processing → Result Sending  
- **Shared Processing Time**: `multiprocessing.Value('d')` for performance metrics sharing
- **Synchronization Event**: `multiprocessing.Event` for initialization coordination

### Detailed Process Architecture

#### 1. Work Requesting Pipeline (Process)
**Purpose**: Manages communication with the producer to acquire new tasks.

**Internal Thread Structure**:
- **ZmqWorkRequester** (Thread): 
  - Sends REQ messages to producer via ZeroMQ REQ socket
  - Handles registration, work requests, and parameter changes
  - Manages outage simulation for testing scenarios
  - Updates shared processing time metrics
- **PipeTaskSender** (Thread):
  - Receives tasks from internal queue
  - Forwards tasks to Task Processing Pipeline via pipe
  - Handles graceful shutdown coordination

**Communication Flow**:
```
Producer ←→ ZmqWorkRequester → Queue → PipeTaskSender → Task Pipe → Task Processing
```

**Thread Coordination**: Uses `Queue.join()` to ensure proper synchronization between network I/O and pipe communication.

#### 2. Task Processing Pipeline (Process)
**Purpose**: Performs the actual computational work (YOLOv11 inference) on received tasks.

**Single-Process Design**: Unlike other pipelines, this runs as a single process (not multi-threaded) to maximize GPU utilization and avoid GIL contention during compute-intensive operations.

**Key Features**:
- **Task Processor Factory**: Creates appropriate processor based on work type (YOLO detection, OBB, etc.)
- **Artificial Capacity Control**: Simulates varying computational capacities for evaluation
- **Processing Time Tracking**: Measures and reports execution times via shared memory
- **Model Management**: Handles inference quality changes (model switching)

**Processing Flow**:
```
Task Pipe → Task Reception → Model Inference → Result Creation → Result Pipe
```

**Initialization Coordination**: Uses `multiprocessing.Event` to signal readiness before work requesting begins.

#### 3. Result Sending Pipeline (Process)
**Purpose**: Transmits processed results to the collector node.

**Internal Thread Structure**:
- **PipeResultReceiver** (Thread):
  - Receives processed tasks from Task Processing Pipeline via pipe
  - Queues results for network transmission
  - Handles end-of-stream coordination
- **ResultSender** (Thread):
  - Sends results to collector via ZeroMQ PUSH socket
  - Manages network connection lifecycle
  - Handles graceful disconnection and error recovery

**Communication Flow**:
```
Result Pipe → PipeResultReceiver → Queue → ResultSender → Collector
```

### Concurrency and Performance Design

#### Process-Level Parallelism
- **True Parallelism**: Each process runs independently, avoiding Python's Global Interpreter Lock (GIL) limitations
- **Pipeline Architecture**: Tasks flow through stages without blocking, maximizing throughput
- **Resource Isolation**: Each process has isolated memory space, preventing interference

#### Thread-Level Coordination  
- **Producer-Consumer Pattern**: Threads within processes use `Queue` objects for safe data exchange
- **Non-Blocking I/O**: Network operations in separate threads prevent computation blocking
- **Graceful Shutdown**: Coordinated shutdown using `TaskType.END` messages propagated through all stages

#### Memory Management
- **Shared Values**: Minimal shared state for performance metrics using `multiprocessing.Value`
- **Pipe Buffers**: Efficient binary data transfer for large NumPy arrays
- **Queue Management**: FIFO queues with `task_done()` acknowledgments for flow control

### Worker Lifecycle

#### 1. Initialization Phase
```python
# Main worker process
worker = Worker(config)
worker.start()

# Registration with producer
request_channel.connect()
work_config = request_channel.register()

# Process creation and pipe setup
task_pipe_recv_end, task_pipe_send_end = Pipe(False)
result_pipe_recv_end, result_pipe_send_end = Pipe(False)
latest_processing_time = Value('d', 0.0)
```

#### 2. Operational Phase
```python
# Start parallel processes
task_processing_pipeline.start()     # Process 1
result_sending_pipeline.start()      # Process 2

# Wait for task processor readiness
task_processor_ready.wait()

# Start work requesting (runs in main process)
work_requesting_pipeline.run()       # Main process
```

#### 3. Shutdown Phase
```python
# Coordinated shutdown via END messages
# Flow: Producer → Work Requesting → Task Processing → Result Sending
task_processing_pipeline.join()
result_sending_pipeline.join()
```

### Key Architectural Benefits

#### 1. **High Throughput**
- Parallel processing stages eliminate bottlenecks
- Network I/O doesn't block computation
- GPU utilization maximized through dedicated compute process

#### 2. **Fault Tolerance**
- Process isolation prevents cascade failures
- Graceful error handling and recovery
- Outage simulation capabilities for testing

#### 3. **Scalability**
- Independent worker processes scale horizontally
- Configurable processing capacities for heterogeneous environments
- Load balancing through producer's work distribution

#### 4. **Resource Efficiency**
- Minimal inter-process communication overhead
- Direct NumPy array transfer via pipes
- Shared memory for lightweight metrics

This sophisticated architecture ensures that each worker can efficiently process tasks while maintaining the responsiveness and reliability required for edge computing scenarios.

## Collector
The collector continuously accepts results from  
The Collector implements a ``zeromq.PULL`` socket that constantly accepts results from workers and aggregates them to produce the final output video-stream.

## Work-API
Implemented by the Producer and used by the Worker.

The Producer implements a `zeromq.ROUTER` socket, that is waiting for requests and returning a task.
The Worker implements a `zeromq.REQ` socket, that is requesting tasks from the Producer's `zeromq.ROUTER` socket.

The communication is request-reply structure:
1. The Worker sends a `REQ`
2. The Producer replies with a `REP`

This is often referred to as the [Load Balancing Pattern](https://zguide.zeromq.org/docs/chapter3/#The-Load-Balancing-Pattern). 
Using this approach, the producer does not decide the distribution of work, rather the decision is made by the worker nodes, as they can choose when to work.
If the workers are configured to request tasks whenever possible, such a design maximises the resource utilization of the workers and therefore the entire distributed system, as each worker requests work up to its own maximum capacity. 
The downside of such an architecture is the added overhead, as the workers have to explicitly request work, whenever their internal buffer is empty.
This overhead is justified as it negligible in comparison to the large amounts of ``Task.data`` that sent via reply of the producer and enables proper load balancing of the worker nodes.

### Task Serialization and Communication
The system uses efficient serialization for network communication between producer and workers:

**Serialization Process (Producer → Worker)**:
1. **Metadata Serialization**: Task metadata (id, type, stream_key, shape, dtype) is serialized using MessagePack
2. **Data Transmission**: NumPy arrays are sent directly using ZeroMQ's buffer interface for maximum efficiency
3. **Multipart Messages**: Each response contains info + (metadata, data) pairs for each task

**Deserialization Process (Worker Side)**:
1. **Metadata Reconstruction**: MessagePack unpacking of task metadata
2. **Array Reconstruction**: NumPy arrays reconstructed from buffer using dtype and shape information
3. **Task Assembly**: Complete Task objects created with all fields including stream_key

**Key Implementation Details**:
- **MessagePack**: Used for metadata serialization due to smaller payload than pickle with similar performance
- **Buffer Interface**: NumPy arrays sent directly without additional serialization overhead
- **Stream Key Preservation**: Ensures proper task routing in multi-stream scenarios
- **Type Safety**: dtype and shape validation during reconstruction prevents data corruption 

### General Request-Reply Structure
`REQ:` A dict that defines the type of request and optional additional information.
```python
req = {
    type: str,
    #(optional additional information)
}
```

`REP:` multipart message containing
General information about the response:
```python
info = {
    type: str,
    #(optional additional information)
}
```
additionally for each task:
```python
task_metadata = {
    id: int,
    dtype: str,
    shape: tuple[int, int, int]
}
```
```python
data: numpy.ndarray
```

So for example a response containing a singular ``Task`` would look like this:
A multipart message containing
```python
info, task_metadata, task_data
``` 
```python
{type: 'WORK'}, {id: 0, type: 'INFERENCE', stream_key: 0, dtype: 'uint8', shape: (1920,1080,3)}, np.ndarray
```

For multiple concurrent streams, the same frame might be sent with different stream keys:
```python
{type: 'WORK'}, {id: 0, type: 'INFERENCE', stream_key: 0, dtype: 'uint8', shape: (1920,1080,3)}, np.ndarray_stream_0,
                 {id: 0, type: 'INFERENCE', stream_key: 1, dtype: 'uint8', shape: (1920,1080,3)}, np.ndarray_stream_1
``` 

### Regular REQ-REP
##### Register -> Confirmation
When a worker starts it sends a registration message to the producer in order to get the current stream configuration.
This makes sure that the worker does not need to be configured individually, solely relying on the provided configuration of the producer.

The worker then loads the required model(s), depending on the retrieved configuration, and adjusts the internal configuration. Upon completing the setup process, the worker starts with the work requesting process
`REQ:` 
```python
req = {
    type='REGISTER'
}
```

`REP:` 
```python
info = {
    type='REGISTRATION_CONFIRMATION',
    work_type: str ∈ ['NONE', 'YOLO_DETECTION', 'YOLO_OBB'],
    work_load: int ∈ [0 (LOW), 1 (MEDIUM), 2 (HIGH)],
    loading_mode: int ∈ [0 (LAZY), 1 (EAGER)]
}
```
##### Request Work -> Receive Work
General mode of operation.

`REQ:` 
```python
req = {
    type='GET_WORK',
    previous_processing_time=0.0 
}
```
``previous_processing_time`` is the processing of the last completed task. This is used by the producer to evaluate SLOs

`REP:` multipart message containing
```python
info = {
    type:str ∈ ['WORK', (Other potential response types)]
}
```
additionally for each task:
```python
task_metadata = {
    type: str,
    id: int,
    stream_key: int,
    dtype: str,
    shape: tuple[int, int, int]
}
```
```python
data: numpy.ndarray
```

##### Request Work -> Receive Change
This happens when there is a crucial change in the current parameter configuration of the stream. The producer replies with a list of changes instead of tasks. 
This is because it is of the utmost importance that the workers adjust to the changes before continuing processing new tasks.

`REQ:` 
```python
req = {
    type='GET_WORK',
    previous_processing_time=0.0
}
```

`REP:`
```python
info = {
    type='CHANGE'
    (optional)  CHANGE_INFERENCE_QUALITY:int ∈ [0 (LOW), 1(MEDIUM), 2 (HIGH)]
    (optional) change-2=value2
    (optional) change-3=value2
    ...
}
```


For example if the work load for the YOLOv11 inference needs to be changed to 1 (MEDIUM) a ``REP`` would look like this:

`REP:`
```python
info = {
    type='CHANGE'
    CHANGE_INFERENCE_QUALITY=1
}
```

### Other Responses
#### Any Rquest -> END of transmission
This occurs when the producer is stopping and has no more tasks left. This signals the workers and by extension the collector to shut down.
```python
req = {
    type=ANY
}
```

`REP:`
```python
info = {
    type='END'
}
```


# Evaluation
# Evaluation

To evaluate the effectiveness of the distributed pipeline and the elasticity control mechanisms, this project implements multiple agent types for comparison and analysis.

## Agent Types

The system implements different elasticity control agents that manage system resources and quality parameters to maintain Service Level Objectives (SLOs) while maximizing Quality of Experience (QoE). Each agent represents a different approach to the resource management problem in edge computing scenarios.

### Active Inference Agent (Relative Control)
The **Active Inference Agent with Relative Control** (`ActiveInferenceAgentRelativeControl`) is the primary intelligent agent of this project, implementing cutting-edge neuroscience-inspired decision-making principles.

#### Key Characteristics:
- **Theoretical Foundation**: Based on the Free Energy Principle from computational neuroscience
- **Learning Capability**: Continuously adapts its internal model of the system through experience
- **Relative Control**: Makes incremental adjustments (increase/decrease/no-change) to stream parameters
- **Probabilistic Reasoning**: Maintains uncertainty estimates and makes decisions under uncertainty
- **Long-term Optimization**: Balances immediate SLO compliance with long-term quality maximization

#### Decision-Making Process:
1. **Bayesian Inference**: Updates beliefs about system state based on observations
2. **Predictive Modeling**: Forecasts likely outcomes of different action sequences
3. **Free Energy Minimization**: Selects actions that minimize surprise and achieve preferred outcomes
4. **Adaptive Learning**: Continuously updates its understanding of how actions affect system performance

#### Advantages:
- Learns optimal policies through experience rather than pre-programmed rules
- Handles uncertainty and partial observability naturally
- Adapts to changing system conditions and workload patterns
- Theoretically grounded approach with strong convergence guarantees
- Can discover complex, non-obvious optimization strategies

#### Use Cases:
- Dynamic environments where optimal strategies are not known a priori
- Systems requiring long-term adaptation and learning
- Scenarios where the relationship between actions and outcomes is complex
- Research applications investigating bio-inspired AI approaches

### Heuristic Agent
The **Heuristic Agent** (`HeuristicAgent`) provides a baseline comparison using traditional rule-based decision-making approaches commonly employed in systems engineering.

#### Key Characteristics:
- **Rule-Based Logic**: Uses predefined decision rules based on engineering heuristics
- **Reactive Control**: Responds directly to current system state without predictive modeling
- **Deterministic Behavior**: Consistent, repeatable decisions given the same input conditions
- **Immediate Response**: Fast decision-making without computational overhead of learning algorithms
- **Oscillation Prevention**: Implements cooldown periods and trend analysis to avoid instability

#### Decision-Making Process:
1. **SLO State Assessment**: Categorizes each SLO as OK, WARNING, or CRITICAL
2. **Priority-Based Response**: 
   - **CRITICAL State**: Immediate parameter reduction to restore SLO compliance
   - **WARNING State**: Proactive adjustments based on trend analysis
   - **OK State**: Quality improvement attempts when resource headroom allows
3. **Balanced Parameter Adjustment**: Uses capacity-based selection to choose which parameter to modify
4. **Trend Analysis**: Tracks SLO history to detect deteriorating conditions early

#### Decision Priority:
When reducing computational load (CRITICAL/WARNING states):
1. Decrease Inference Quality (highest computational impact)
2. Decrease FPS (moderate computational impact)  
3. Decrease Resolution (lowest computational impact)

When improving quality (OK states):
1. Increase Resolution (highest user experience impact)
2. Increase FPS (moderate user experience impact)
3. Increase Inference Quality (variable impact based on use case)

#### Oscillation Prevention Mechanisms:
- **Cooldown Periods**: Prevents rapid successive changes that could destabilize the system
- **Trend-Based Decisions**: Uses historical SLO values to detect patterns and avoid premature reactions
- **Capacity Balancing**: Ensures parameter changes are distributed appropriately across the quality dimensions
- **Action History Tracking**: Prevents alternating between opposite actions

#### Advantages:
- Fast, deterministic response times
- Predictable and interpretable behavior
- No computational overhead for learning or model updates
- Robust performance in well-understood scenarios
- Easy to tune and modify for specific requirements

#### Use Cases:
- Production systems requiring predictable behavior
- Baseline comparison for evaluating learning-based approaches
- Scenarios with well-understood system dynamics
- Resource-constrained environments where computational overhead must be minimized

### Comparative Analysis

| Aspect | Active Inference Agent | Heuristic Agent |
|--------|----------------------|-----------------|
| **Learning** | Continuous adaptation through experience | Static rule-based behavior |
| **Complexity** | High computational overhead, complex internal model | Low overhead, simple logic |
| **Adaptability** | Excellent - discovers optimal strategies over time | Limited - requires manual tuning |
| **Predictability** | Variable - behavior evolves with learning | High - deterministic rule execution |
| **Initialization** | May require exploration period for optimal performance | Immediate effective operation |
| **Theoretical Foundation** | Neuroscience-inspired, mathematically rigorous | Engineering heuristics, empirically validated |
| **Robustness** | Handles novel scenarios through learning | Limited to scenarios covered by rules |

Both agents share the same fundamental objectives:
1. **Primary Goal**: Maintain all SLO constraints (Queue Size, Memory Usage, Processing Times)
2. **Secondary Goal**: Maximize stream quality parameters (Resolution, FPS, Inference Quality) within SLO bounds
3. **Operational Goal**: Ensure stable system operation without oscillations or instability

The comparison between these approaches provides insights into the trade-offs between traditional systems engineering approaches and modern AI-based solutions in edge computing scenarios.

## Simulation Methodology

To comprehensively evaluate the effectiveness of different elasticity control agents, this project implements a sophisticated simulation framework with multiple test scenarios. The evaluation compares agent performance across different operational conditions to understand their strengths, weaknesses, and adaptation capabilities.

### Experimental Environment
All simulations run on a controlled single-machine environment to ensure reproducible results:

- **Operating System:** Windows 11, Version 23H2 (Build 22631.5335)
- **Python Runtime:** Python 3.12.2
- **CPU:** AMD Ryzen 7 7800X3D (8 cores, 16 threads)
- **GPU:** Nvidia GeForce GTX 1660Ti (MSI GTX Ti Ventus XS OC)
- **Memory:** 32GB DDR5 RAM, Clock: 4800MHz, Dual Channel
- **Storage:** WD Black SN770 2TB NVMe SSD

### System Configuration
The distributed pipeline setup consists of:
- **1 Producer Node**: Generates video stream tasks and controls elasticity
- **3 Worker Nodes**: Process YOLOv11 inference tasks (configurable capacities)
- **1 Collector Node**: Aggregates processed results into output stream

**Video Dataset**: Highway traffic video stream used for vehicle detection via YOLOv11 inference. The video provides realistic computational workload with varying scene complexity throughout the timeline.

**Worker Configuration**: Each worker has configurable processing capacity (0.0-1.0 scale) representing their computational power relative to the baseline. Different capacity configurations test system behavior under heterogeneous resource availability.

### Simulation Test Cases

The evaluation framework implements three distinct simulation scenarios to test different aspects of elasticity control:

#### 1. Base Case Simulation (`SimulationType.BASIC`)
**Purpose**: Establish baseline performance under stable conditions.

**Configuration**:
- **Timeline**: Complete video processing from start to finish
- **Workers**: 3 workers with capacities [0.6, 0.5, 0.4] representing heterogeneous edge nodes
- **Computational Load**: Constant single video stream (multiplier = 1)
- **Conditions**: No external perturbations or resource changes

**Expected Behavior**: 
- Agents should maintain high quality parameters while keeping all SLOs satisfied
- Minimal elasticity adjustments needed
- Steady-state operation after initial optimization period

#### 2. Variable Computational Budget Simulation (`SimulationType.VARIABLE_COMPUTATIONAL_BUDGET`)
**Purpose**: Test agent response to sudden resource availability changes (worker outages/recovery).

**Configuration**:
- **Timeline**: Video processing with scheduled worker outages
- **Workers**: 
  - 1regular worker (capacity 0.5) - remain online throughout
  - 1 outage worker (capacity 0.5) - simulate temporary failures
- **Outage Schedule**:
  - **0-33%**: All workers operational (full computational budget)
  - **33-66%**: Outage workers go offline (reduced computational budget)
  - **66-100%**: Outage workers return online (restored computational budget)

**Expected Behavior**:
- **33% mark**: Agents should detect resource reduction and proactively reduce quality parameters
- **66% mark**: Agents should detect resource restoration and gradually increase quality parameters
- Demonstrates adaptation to infrastructure failures/recovery scenarios

#### 3. Variable Computational Demand Simulation (`SimulationType.VARIABLE_COMPUTATIONAL_DEMAND`)
**Purpose**: Test agent response to changing computational workload (multiple concurrent streams).

**Configuration**:
- **Timeline**: Video processing with scheduled demand increases
- **Workers**: 3 workers with higher capacities [0.8, 0.75, 0.7] to handle increased load
- **Demand Schedule**:
  - **0-25%**: Single stream (multiplier = 1, baseline load)
  - **25-50%**: Triple stream (multiplier = 3, 3x computational demand)
  - **50%-75%**: Double stream (multiplier = 2, 2x computational demand)
  - **75-100%**: Return to single stream (multiplier = 1)

**Stream Multiplier Mechanism**: Simulates multiple concurrent video streams by replicating each frame N times, creating N identical tasks for the same video frame. This realistically increases computational demand without requiring multiple video sources.

**Expected Behavior**:
- **25% mark**: Agents should detect increased demand and preemptively reduce quality parameters
- **75% mark**: Agents should detect reduced demand and restore higher quality parameters
- Tests scalability and load management capabilities

### Metrics and Performance Analysis

The evaluation framework captures comprehensive performance metrics across multiple dimensions:

#### Service Level Objective (SLO) Metrics
**Primary SLO Fulfillment Metrics**:
- **Overall SLO Fulfillment Rate**: Percentage of time when ALL 4 SLOs are satisfied simultaneously
- **Average SLO Fulfillment Rate**: Mean fulfillment rate across individual SLOs
- **Individual SLO Fulfillment Rates**: Separate rates for each SLO:
  - Queue Size SLO Fulfillment Rate
  - Memory Usage SLO Fulfillment Rate  
  - Global Processing Time SLO Fulfillment Rate
  - Worker Processing Time SLO Fulfillment Rate

**SLO Violation Analysis**:
- **Violation Severity**: How much SLOs are exceeded when violated (average and maximum)
- **Consecutive Violations**: Maximum number of consecutive time steps with ANY SLO violation
- **Stability Coefficients**: Standard deviation relative to mean for each SLO (measures oscillation)

#### Quality of Experience (QoE) Metrics
- **Average Stream Quality**: Combined score across all quality dimensions (FPS, Resolution, Inference Quality)
- **Quality Parameter Utilization**: Individual capacity utilization for each quality dimension
- **Quality Stability**: Variance in quality parameters over time

#### Statistical Performance Metrics
- **SLO Value Statistics**: Mean values for each SLO across the simulation
- **System Stability**: Coefficient of variation for key performance indicators
- **Recovery Performance**: Time to restore quality after disturbances
- **Adaptation Efficiency**: Rate of parameter adjustment in response to changes

### Visualization and Analysis

The evaluation system generates comprehensive visualizations using matplotlib and seaborn:

#### SLO Performance Plots
- **SLO Values Over Time**: Time-series plots showing all 4 SLO values with critical threshold (1.0) marked
- **SLO Fulfillment Timeline**: Binary visualization of when SLOs are satisfied/violated
- **SLO Distribution Analysis**: Histograms showing distribution of SLO values across simulation

#### Quality Metrics Visualization  
- **Quality Parameters Over Time**: Time-series showing FPS, Resolution, and Inference Quality capacity
- **Quality Utilization Heatmaps**: Visual representation of parameter utilization patterns
- **Quality-SLO Correlation**: Scatter plots showing relationship between quality settings and SLO performance

#### Worker Performance Analysis
- **Task Distribution**: Pie charts and bar graphs showing workload distribution among workers
- **Worker Utilization**: Individual worker performance and task processing rates
- **Load Balancing Effectiveness**: Analysis of how work is distributed across heterogeneous workers

#### Comparative Analysis
- **Agent Comparison**: Side-by-side performance comparison between Active Inference and Heuristic agents
- **Simulation Scenario Analysis**: Performance comparison across different test cases
- **Metric Correlation Analysis**: Understanding relationships between different performance dimensions

### Output and Reporting

**Automated Output Generation**:
- **Metrics Files**: JSON files containing all calculated performance metrics for each agent-simulation combination
- **Visualization Exports**: High-resolution PNG plots organized by simulation type and agent
- **Directory Structure**: Organized output in `out/` directory with subdirectories for each simulation type

**File Naming Convention**:
- Metrics: `{agent_type}_metrics.json`
- Plots: `{agent_type}_{plot_type}.png`
- Organization: `out/{simulation_type}_sim/` directories

This comprehensive evaluation framework enables systematic comparison of elasticity control approaches and provides quantitative evidence for the effectiveness of Active Inference-based agents in edge computing scenarios.

# Implementation
## Compute Platform
Nvidia [CUDA 12.8](https://developer.nvidia.com/cuda-12-8-0-download-archive) and the respective [pytorch](https://download.pytorch.org/whl/cu128) version
## Inference Model
Ultralytics [YOLOv11](https://docs.ultralytics.com/models/yolo11/)
## Network Communication 
python ZeroMQ implementation [pyzmq](https://zeromq.org/languages/python/). 
Supports effective sending of numpy arrays using this [functionality](https://pyzmq.readthedocs.io/en/latest/howto/serialization.html#example-numpy-arrays).
## Active Inference Library
[pymdp](https://github.com/infer-actively/pymdp)
## Causel Inference via DAG
[pgmpy](https://pgmpy.org/) used in in [intelligentVehicle](https://github.com/borissedlak/intelligentVehicle)
## Serialization
[msgpack](https://msgpack.org/index.html) because the result is slightly smaller than [pickle](https://docs.python.org/3/library/pickle.html) and has similar performance in serialization speed for this use case.
## Reinforcement Learning (RL) Library
[stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/)
## Plotting
[matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/)
## Date collection
[Pandas](https://pandas.pydata.org/)

# Dependencies
## How to build `requirements.txt`
```pip
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip install ultralytics
pip install msgpack
pip install pyzmq
pip install git+https://github.com/infer-actively/pymdp.git


pip freeze > requirements.txt
```

Do not install pymdp via ``pip install inferactively-pymdp``. This is not being maintained (Last update 2023)

Add on top of requirements.txt
```
-f https://download.pytorch.org/whl/cu126/torch
-f https://download.pytorch.org/whl/cu126/torchvision
-f https://download.pytorch.org/whl/cu126/torchaudio
```




# TODO
Optional:
    When changing fps: notify the collector that the fps are changed
    Send multiple frames at once Producer -> Worker && Worker -> Collector

